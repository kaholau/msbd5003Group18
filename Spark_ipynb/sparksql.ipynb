{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from pyspark import SparkContext\n",
    "import sys\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "#sc = SparkContext()\n",
    "spark = SQLContext(sc)\n",
    "row = Row(name=\"Alice\", age=11)\n",
    "print row\n",
    "print row['name'], row['age']\n",
    "print row.name, row.age\n",
    "\n",
    "row = Row(name=\"Alice\", age=11, count=1)\n",
    "print row.count\n",
    "print row['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data file at https://www.cse.ust.hk/msbd/data/building.csv\n",
    "\n",
    "df = spark.read.csv('building.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+------------+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "|         1|         M1|         25|     AC1000|         USA|\n",
      "|         2|         M2|         27|     FN39TG|      France|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|\n",
      "|         4|         M4|         17|     GG1919|     Finland|\n",
      "|         5|         M5|          3|    ACMAX22|   Hong Kong|\n",
      "|         6|         M6|          9|     AC1000|   Singapore|\n",
      "|         7|         M7|         13|     FN39TG|South Africa|\n",
      "|         8|         M8|         25|     JDNS77|   Australia|\n",
      "|         9|         M9|         11|     GG1919|      Mexico|\n",
      "|        10|        M10|         23|    ACMAX22|       China|\n",
      "|        11|        M11|         14|     AC1000|     Belgium|\n",
      "|        12|        M12|         26|     FN39TG|     Finland|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia|\n",
      "|        14|        M14|         17|     GG1919|     Germany|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel|\n",
      "|        16|        M16|         23|     AC1000|      Turkey|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia|\n",
      "|        19|        M19|         14|     GG1919|      Canada|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the content of the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BuildingID: integer (nullable = true)\n",
      " |-- BuildingMgr: string (nullable = true)\n",
      " |-- BuildingAge: integer (nullable = true)\n",
      " |-- HVACproduct: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the dataframe schema in a tree format\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(BuildingID=1, BuildingMgr=u'M1', BuildingAge=25, HVACproduct=u'AC1000', Country=u'USA'), Row(BuildingID=2, BuildingMgr=u'M2', BuildingAge=27, HVACproduct=u'FN39TG', Country=u'France'), Row(BuildingID=3, BuildingMgr=u'M3', BuildingAge=28, HVACproduct=u'JDNS77', Country=u'Brazil')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an RDD from the dataframe\n",
    "dfrdd = df.rdd\n",
    "dfrdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|BuildingID|     Country|\n",
      "+----------+------------+\n",
      "|         1|         USA|\n",
      "|         2|      France|\n",
      "|         3|      Brazil|\n",
      "|         4|     Finland|\n",
      "|         5|   Hong Kong|\n",
      "|         6|   Singapore|\n",
      "|         7|South Africa|\n",
      "|         8|   Australia|\n",
      "|         9|      Mexico|\n",
      "|        10|       China|\n",
      "|        11|     Belgium|\n",
      "|        12|     Finland|\n",
      "|        13|Saudi Arabia|\n",
      "|        14|     Germany|\n",
      "|        15|      Israel|\n",
      "|        16|      Turkey|\n",
      "|        17|       Egypt|\n",
      "|        18|   Indonesia|\n",
      "|        19|      Canada|\n",
      "|        20|   Argentina|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve specific columns from the dataframe\n",
    "df.select('BuildingID', 'Country').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+-------+---+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|Country| OK|\n",
      "+----------+-----------+-----------+-----------+-------+---+\n",
      "|         1|         M1|         25|     AC1000|    USA| OK|\n",
      "+----------+-----------+-----------+-----------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df.where(\"Country='USA'\").select('*', lit('OK')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|HVACProduct|count|\n",
      "+-----------+-----+\n",
      "|    ACMAX22|    4|\n",
      "|     AC1000|    4|\n",
      "|     JDNS77|    4|\n",
      "|     FN39TG|    4|\n",
      "|     GG1919|    4|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use GroupBy clause with dataframe \n",
    "df.groupBy('HVACProduct').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewriting SQL with DataFrame API\n",
    "\n",
    "The data files have been put to a public blob container, which can be accessed as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data from csv files\n",
    "# Data files at https://www.cse.ust.hk/msbd5003/data\n",
    "\n",
    "dfCustomer = spark.read.csv('../data/Customer.csv', header=True, inferSchema=True)\n",
    "dfProduct = spark.read.csv('../data/Product.csv', header=True, inferSchema=True)\n",
    "dfDetail = spark.read.csv('../data/SalesOrderDetail.csv', header=True, inferSchema=True)\n",
    "dfHeader = spark.read.csv('../data/SalesOrderHeader.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+---------+\n",
      "|ProductID|Name                         |ListPrice|\n",
      "+---------+-----------------------------+---------+\n",
      "|680      |HL Road Frame - Black, 58    |1431.5   |\n",
      "|708      |Sport-100 Helmet, Black      |34.99    |\n",
      "|722      |LL Road Frame - Black, 58    |337.22   |\n",
      "|723      |LL Road Frame - Black, 60    |337.22   |\n",
      "|724      |LL Road Frame - Black, 62    |337.22   |\n",
      "|736      |LL Road Frame - Black, 44    |337.22   |\n",
      "|737      |LL Road Frame - Black, 48    |337.22   |\n",
      "|738      |LL Road Frame - Black, 52    |337.22   |\n",
      "|743      |HL Mountain Frame - Black, 42|1349.6   |\n",
      "|744      |HL Mountain Frame - Black, 44|1349.6   |\n",
      "|745      |HL Mountain Frame - Black, 48|1349.6   |\n",
      "|746      |HL Mountain Frame - Black, 46|1349.6   |\n",
      "|747      |HL Mountain Frame - Black, 38|1349.6   |\n",
      "|765      |Road-650 Black, 58           |782.99   |\n",
      "|766      |Road-650 Black, 60           |782.99   |\n",
      "|767      |Road-650 Black, 62           |782.99   |\n",
      "|768      |Road-650 Black, 44           |782.99   |\n",
      "|769      |Road-650 Black, 48           |782.99   |\n",
      "|770      |Road-650 Black, 52           |782.99   |\n",
      "|775      |Mountain-100 Black, 38       |3374.99  |\n",
      "+---------+-----------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SELECT ProductID, Name, ListPrice \n",
    "# FROM Product \n",
    "# WHERE Color = 'black'\n",
    "\n",
    "dfProduct.filter(\"Color = 'Black'\")\\\n",
    "         .select('ProductID', 'Name', 'ListPrice')\\\n",
    "         .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+-----------+\n",
      "|ProductID|Name                         |DoublePrice|\n",
      "+---------+-----------------------------+-----------+\n",
      "|680      |HL Road Frame - Black, 58    |2863.0     |\n",
      "|708      |Sport-100 Helmet, Black      |69.98      |\n",
      "|722      |LL Road Frame - Black, 58    |674.44     |\n",
      "|723      |LL Road Frame - Black, 60    |674.44     |\n",
      "|724      |LL Road Frame - Black, 62    |674.44     |\n",
      "|736      |LL Road Frame - Black, 44    |674.44     |\n",
      "|737      |LL Road Frame - Black, 48    |674.44     |\n",
      "|738      |LL Road Frame - Black, 52    |674.44     |\n",
      "|743      |HL Mountain Frame - Black, 42|2699.2     |\n",
      "|744      |HL Mountain Frame - Black, 44|2699.2     |\n",
      "|745      |HL Mountain Frame - Black, 48|2699.2     |\n",
      "|746      |HL Mountain Frame - Black, 46|2699.2     |\n",
      "|747      |HL Mountain Frame - Black, 38|2699.2     |\n",
      "|765      |Road-650 Black, 58           |1565.98    |\n",
      "|766      |Road-650 Black, 60           |1565.98    |\n",
      "|767      |Road-650 Black, 62           |1565.98    |\n",
      "|768      |Road-650 Black, 44           |1565.98    |\n",
      "|769      |Road-650 Black, 48           |1565.98    |\n",
      "|770      |Road-650 Black, 52           |1565.98    |\n",
      "|775      |Mountain-100 Black, 38       |6749.98    |\n",
      "+---------+-----------------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfProduct.where(dfProduct.Color=='Black') \\\n",
    "         .select(dfProduct.ProductID, dfProduct['Name'], (dfProduct.ListPrice * 2).alias('DoublePrice')) \\\n",
    "         .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+---------------+\n",
      "|ProductID|Name                     |(ListPrice * 2)|\n",
      "+---------+-------------------------+---------------+\n",
      "|680      |HL Road Frame - Black, 58|2863.0         |\n",
      "|706      |HL Road Frame - Red, 58  |2863.0         |\n",
      "|717      |HL Road Frame - Red, 62  |2863.0         |\n",
      "|718      |HL Road Frame - Red, 44  |2863.0         |\n",
      "|719      |HL Road Frame - Red, 48  |2863.0         |\n",
      "|720      |HL Road Frame - Red, 52  |2863.0         |\n",
      "|721      |HL Road Frame - Red, 56  |2863.0         |\n",
      "|722      |LL Road Frame - Black, 58|674.44         |\n",
      "|723      |LL Road Frame - Black, 60|674.44         |\n",
      "|724      |LL Road Frame - Black, 62|674.44         |\n",
      "|725      |LL Road Frame - Red, 44  |674.44         |\n",
      "|726      |LL Road Frame - Red, 48  |674.44         |\n",
      "|727      |LL Road Frame - Red, 52  |674.44         |\n",
      "|728      |LL Road Frame - Red, 58  |674.44         |\n",
      "|729      |LL Road Frame - Red, 60  |674.44         |\n",
      "|730      |LL Road Frame - Red, 62  |674.44         |\n",
      "|731      |ML Road Frame - Red, 44  |1189.66        |\n",
      "|732      |ML Road Frame - Red, 48  |1189.66        |\n",
      "|733      |ML Road Frame - Red, 52  |1189.66        |\n",
      "|734      |ML Road Frame - Red, 58  |1189.66        |\n",
      "+---------+-------------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfProduct.where(dfProduct.ListPrice * 2 > 100) \\\n",
    "         .select(dfProduct.ProductID, dfProduct['Name'], dfProduct.ListPrice * 2) \\\n",
    "         .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------+---------+\n",
      "|ProductID|Name                      |ListPrice|\n",
      "+---------+--------------------------+---------+\n",
      "|860      |Half-Finger Gloves, L     |24.49    |\n",
      "|859      |Half-Finger Gloves, M     |24.49    |\n",
      "|858      |Half-Finger Gloves, S     |24.49    |\n",
      "|708      |Sport-100 Helmet, Black   |34.99    |\n",
      "|862      |Full-Finger Gloves, M     |37.99    |\n",
      "|861      |Full-Finger Gloves, S     |37.99    |\n",
      "|863      |Full-Finger Gloves, L     |37.99    |\n",
      "|841      |Men's Sports Shorts, S    |59.99    |\n",
      "|849      |Men's Sports Shorts, M    |59.99    |\n",
      "|851      |Men's Sports Shorts, XL   |59.99    |\n",
      "|850      |Men's Sports Shorts, L    |59.99    |\n",
      "|815      |LL Mountain Front Wheel   |60.745   |\n",
      "|868      |Women's Mountain Shorts, M|69.99    |\n",
      "|869      |Women's Mountain Shorts, L|69.99    |\n",
      "|867      |Women's Mountain Shorts, S|69.99    |\n",
      "|853      |Women's Tights, M         |74.99    |\n",
      "|854      |Women's Tights, L         |74.99    |\n",
      "|852      |Women's Tights, S         |74.99    |\n",
      "|818      |LL Road Front Wheel       |85.565   |\n",
      "|823      |LL Mountain Rear Wheel    |87.745   |\n",
      "+---------+--------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SELECT ProductID, Name, ListPrice \n",
    "# FROM Product \n",
    "# WHERE Color = 'black' \n",
    "# ORDER BY ProductID\n",
    "\n",
    "dfProduct.filter(\"Color = 'Black'\")\\\n",
    "         .select('ProductID', 'Name', 'ListPrice')\\\n",
    "         .orderBy('ListPrice')\\\n",
    "         .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+--------------------+---------+--------+\n",
      "|SalesOrderID|SalesOrderDetailID|                Name|UnitPrice|OrderQty|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|       71780|            110620|Mountain-500 Blac...|  323.994|       1|\n",
      "|       71780|            110621|LL Mountain Frame...|  149.874|       1|\n",
      "|       71780|            110622|HL Mountain Frame...|   809.76|       1|\n",
      "|       71780|            110623|Mountain-200 Blac...| 1376.994|       4|\n",
      "|       71780|            110627|Women's Mountain ...|   41.994|       6|\n",
      "|       71780|            110629|Mountain-500 Blac...|  323.994|       2|\n",
      "|       71780|            110630|Mountain-500 Blac...|  323.994|       3|\n",
      "|       71780|            110631|Mountain-500 Blac...|  323.994|       1|\n",
      "|       71780|            110632|Mountain-500 Blac...|  323.994|       2|\n",
      "|       71780|            110638|Mountain-200 Blac...| 1376.994|       5|\n",
      "|       71780|            110642|LL Mountain Frame...|  149.874|       1|\n",
      "|       71780|            110643|Women's Mountain ...|   41.994|       7|\n",
      "|       71782|            110690|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71782|            110697|         HL Crankset|  242.994|       2|\n",
      "|       71782|            110705|Half-Finger Glove...|   14.694|       1|\n",
      "|       71783|            110710|LL Road Frame - B...|  202.332|       4|\n",
      "|       71783|            110712|  Road-250 Black, 44|  1466.01|       3|\n",
      "|       71783|            110713|  Road-750 Black, 58|  323.994|       4|\n",
      "|       71783|            110715|Half-Finger Glove...|   14.694|       7|\n",
      "|       71783|            110725|Half-Finger Glove...|   14.694|       2|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find all orders and details on black product,\n",
    "# return the product SalesOrderID, SalesOrderDetailID, Name, UnitPrice, and OrderQty\n",
    "\n",
    "# SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty \n",
    "# FROM SalesLT.SalesOrderDetail, SalesLT.Product\n",
    "# WHERE SalesOrderDetail.ProductID = Product.ProductID AND Color = 'Black'\n",
    "\n",
    "# SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty \n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID\n",
    "# WHERE Color = 'Black'\n",
    "\n",
    "# Spark SQL supports natural joins\n",
    "\n",
    "dfDetail.join(dfProduct, 'ProductID') \\\n",
    "        .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty') \\\n",
    "        .filter(\"Color='Black'\")\\\n",
    "        .show()\n",
    "\n",
    "# If we move the filter to after select, it still works.  Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+--------------------+---------+--------+\n",
      "|SalesOrderID|SalesOrderDetailID|                Name|UnitPrice|OrderQty|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|       71780|            110620|Mountain-500 Blac...|  323.994|       1|\n",
      "|       71780|            110621|LL Mountain Frame...|  149.874|       1|\n",
      "|       71780|            110622|HL Mountain Frame...|   809.76|       1|\n",
      "|       71780|            110623|Mountain-200 Blac...| 1376.994|       4|\n",
      "|       71780|            110627|Women's Mountain ...|   41.994|       6|\n",
      "|       71780|            110629|Mountain-500 Blac...|  323.994|       2|\n",
      "|       71780|            110630|Mountain-500 Blac...|  323.994|       3|\n",
      "|       71780|            110631|Mountain-500 Blac...|  323.994|       1|\n",
      "|       71780|            110632|Mountain-500 Blac...|  323.994|       2|\n",
      "|       71780|            110638|Mountain-200 Blac...| 1376.994|       5|\n",
      "|       71780|            110642|LL Mountain Frame...|  149.874|       1|\n",
      "|       71780|            110643|Women's Mountain ...|   41.994|       7|\n",
      "|       71782|            110690|Sport-100 Helmet,...|   20.994|       7|\n",
      "|       71782|            110697|         HL Crankset|  242.994|       2|\n",
      "|       71782|            110705|Half-Finger Glove...|   14.694|       1|\n",
      "|       71783|            110710|LL Road Frame - B...|  202.332|       4|\n",
      "|       71783|            110712|  Road-250 Black, 44|  1466.01|       3|\n",
      "|       71783|            110713|  Road-750 Black, 58|  323.994|       4|\n",
      "|       71783|            110715|Half-Finger Glove...|   14.694|       7|\n",
      "|       71783|            110725|Half-Finger Glove...|   14.694|       2|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*Project [SalesOrderID#227, SalesOrderDetailID#228, Name#181, UnitPrice#231, OrderQty#229]\n",
      "+- *BroadcastHashJoin [ProductID#230], [ProductID#180], Inner, BuildRight\n",
      "   :- *Project [SalesOrderID#227, SalesOrderDetailID#228, OrderQty#229, ProductID#230, UnitPrice#231]\n",
      "   :  +- *Filter isnotnull(ProductID#230)\n",
      "   :     +- *FileScan csv [SalesOrderID#227,SalesOrderDetailID#228,OrderQty#229,ProductID#230,UnitPrice#231] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/csproject/msbd5003/public_html/data/SalesOrderDetail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,SalesOrderDetailID:int,OrderQty:int,ProductID:int,UnitPrice:double>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n",
      "      +- *Project [ProductID#180, Name#181]\n",
      "         +- *Filter ((isnotnull(Color#183) && (Color#183 = Black)) && isnotnull(ProductID#180))\n",
      "            +- *FileScan csv [ProductID#180,Name#181,Color#183] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/csproject/msbd5003/public_html/data/Product.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Color), EqualTo(Color,Black), IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Name:string,Color:string>\n"
     ]
    }
   ],
   "source": [
    "# This also works:\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID') \\\n",
    "             .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty')\n",
    "d2 = d1.filter(\"Color = 'Black'\")\n",
    "d2.show()\n",
    "d2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Py4JJavaError: An error occurred while calling o190.csv.\n",
       ": java.io.IOException: No FileSystem for scheme: wasb\n",
       "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n",
       "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n",
       "\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n",
       "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n",
       "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n",
       "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n",
       "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:407)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:474)\n",
       "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:598)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling o190.csv.\\n', JavaObject id=o191), <traceback object at 0x7faf31cd42d8>)\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will report an error:\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID') \\\n",
    "             .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty')\n",
    "d1.write.csv('wasb:///temp.csv', mode = 'overwrite', header = True)\n",
    "d2 = spark.read.csv('wasb:///temp.csv', header = True, inferSchema = True)\n",
    "d2.filter(\"Color = 'Black'\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|SalesOrderID|\n",
      "+------------+\n",
      "|       71902|\n",
      "|       71832|\n",
      "|       71915|\n",
      "|       71831|\n",
      "|       71898|\n",
      "|       71935|\n",
      "|       71938|\n",
      "|       71845|\n",
      "|       71783|\n",
      "|       71815|\n",
      "|       71936|\n",
      "|       71863|\n",
      "|       71780|\n",
      "|       71782|\n",
      "|       71899|\n",
      "|       71784|\n",
      "|       71797|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find all orders that include at least one black product, \n",
    "# return the product SalesOrderID, Name, UnitPrice, and OrderQty\n",
    "\n",
    "# SELECT DISTINCT SalesOrderID\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID\n",
    "# WHERE Color = 'Black'\n",
    "\n",
    "dfDetail.join(dfProduct.filter(\"Color='Black'\"), 'ProductID') \\\n",
    "        .select('SalesOrderID') \\\n",
    "        .distinct() \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many colors in the products?\n",
    "\n",
    "# SELECT COUNT(DISTINCT Color)\n",
    "# FROM SalesLT.Product\n",
    "\n",
    "dfProduct.select('Color').distinct().count()\n",
    "\n",
    "# It's 1 more than standard SQL.  In standard SQL, COUNT() does not count NULLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|SalesOrderID|        TotalPrice|\n",
      "+------------+------------------+\n",
      "|       71867|             858.9|\n",
      "|       71902|59894.209199999976|\n",
      "|       71832|      28950.678108|\n",
      "|       71915|1732.8899999999999|\n",
      "|       71946|            31.584|\n",
      "|       71895|221.25600000000003|\n",
      "|       71816|2847.4079999999994|\n",
      "|       71831|          1712.946|\n",
      "|       71923|         96.108824|\n",
      "|       71858|11528.844000000001|\n",
      "|       71917|            37.758|\n",
      "|       71897|          10585.05|\n",
      "|       71885|           524.664|\n",
      "|       71856|500.30400000000003|\n",
      "|       71898| 53248.69200000002|\n",
      "|       71774|           713.796|\n",
      "|       71796| 47848.02600000001|\n",
      "|       71935|5533.8689079999995|\n",
      "|       71938|         74160.228|\n",
      "|       71845|        34118.5356|\n",
      "+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total price of each order, \n",
    "# return SalesOrderID and total price (column name should be ‘totalprice’)\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# GROUP BY SalesOrderID\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail.OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|SalesOrderID|        TotalPrice|\n",
      "+------------+------------------+\n",
      "|       71902|59894.209199999976|\n",
      "|       71832|      28950.678108|\n",
      "|       71858|11528.844000000001|\n",
      "|       71897|          10585.05|\n",
      "|       71898| 53248.69200000002|\n",
      "|       71796| 47848.02600000001|\n",
      "|       71938|         74160.228|\n",
      "|       71845|        34118.5356|\n",
      "|       71783|      65683.367986|\n",
      "|       71936| 79589.61602399996|\n",
      "|       71780|29923.007999999998|\n",
      "|       71782| 33319.98600000001|\n",
      "|       71784| 89869.27631400003|\n",
      "|       71797| 65123.46341800001|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total price of each order where the total price > 10000\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# GROUP BY SalesOrderID\n",
    "# HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .where('TotalPrice > 10000')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|SalesOrderID|        TotalPrice|\n",
      "+------------+------------------+\n",
      "|       71902|         26677.884|\n",
      "|       71832|      16883.748108|\n",
      "|       71938|         33779.448|\n",
      "|       71845|         18109.836|\n",
      "|       71783|15524.117476000003|\n",
      "|       71936| 44490.29042399999|\n",
      "|       71780|16964.321999999996|\n",
      "|       71797|      27581.613792|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total price on the black products of each order where the total price > 10000\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail, SalesLT.Product\n",
    "# WHERE SalesLT.SalesOrderDetail.ProductID = SalesLT.Product.ProductID AND Color = 'Black'\n",
    "# GROUP BY SalesOrderID\n",
    "# HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .join(dfProduct.where(\"Color = 'Black'\"), 'ProductID') \\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .where('TotalPrice > 10000')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+-------------+\n",
      "|CustomerID|   FirstName|    LastName|sum(OrderQty)|\n",
      "+----------+------------+------------+-------------+\n",
      "|     30050|     Krishna|Sunkammurali|           89|\n",
      "|     29796|         Jon|      Grande|           65|\n",
      "|     29957|       Kevin|         Liu|           62|\n",
      "|     29929|     Jeffrey|       Kurtz|           46|\n",
      "|     29546| Christopher|        Beck|           45|\n",
      "|     29922|      Pamala|        Kotc|           34|\n",
      "|     30113|        Raja|   Venugopal|           34|\n",
      "|     29938|       Frank|    Campbell|           29|\n",
      "|     29736|       Terry|   Eminhizer|           23|\n",
      "|     29485|   Catherine|        Abel|           10|\n",
      "|     30019|     Matthew|      Miller|            9|\n",
      "|     29932|     Rebecca|      Laszlo|            7|\n",
      "|     29975|      Walter|        Mays|            5|\n",
      "|     29638|    Rosmarie|     Carroll|            2|\n",
      "|     30089|Michael John|      Troyer|            1|\n",
      "|     29568|      Donald|     Blanton|            1|\n",
      "|     29531|        Cory|       Booth|            1|\n",
      "+----------+------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each customer, find the total quantity of black products bought.\n",
    "# Report CustomerID, FirstName, LastName, and total quantity\n",
    "\n",
    "# select saleslt.customer.customerid, FirstName, LastName, sum(orderqty)\n",
    "# from saleslt.customer\n",
    "# left outer join \n",
    "# (\n",
    "# saleslt.salesorderheader\n",
    "# join saleslt.salesorderdetail\n",
    "# on saleslt.salesorderdetail.salesorderid = saleslt.salesorderheader.salesorderid\n",
    "# join saleslt.product\n",
    "# on saleslt.product.productid = saleslt.salesorderdetail.productid and color = 'black'\n",
    "# )\n",
    "# on saleslt.customer.customerid = saleslt.salesorderheader.customerid\n",
    "# group by saleslt.customer.customerid, FirstName, LastName\n",
    "# order by sum(orderqty) desc\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID')\\\n",
    "             .where('Color = \"Black\"') \\\n",
    "             .join(dfHeader, 'SalesOrderID')\\\n",
    "             .groupBy('CustomerID').sum('OrderQty')\n",
    "dfCustomer.join(d1, 'CustomerID', 'left_outer')\\\n",
    "          .select('CustomerID', 'FirstName', 'LastName', 'sum(OrderQty)')\\\n",
    "          .orderBy('sum(OrderQty)', ascending=False)\\\n",
    "          .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "### Embed SQL queries\n",
    "\n",
    "You can also run SQL queries over dataframes once you register them as temporary tables within the SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Register the dataframe as a temporary view called HVAC\n",
    "df.createOrReplaceTempView('HVAC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+------------+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "|         1|         M1|         25|     AC1000|         USA|\n",
      "|         2|         M2|         27|     FN39TG|      France|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|\n",
      "|         4|         M4|         17|     GG1919|     Finland|\n",
      "|         7|         M7|         13|     FN39TG|South Africa|\n",
      "|         8|         M8|         25|     JDNS77|   Australia|\n",
      "|         9|         M9|         11|     GG1919|      Mexico|\n",
      "|        10|        M10|         23|    ACMAX22|       China|\n",
      "|        11|        M11|         14|     AC1000|     Belgium|\n",
      "|        12|        M12|         26|     FN39TG|     Finland|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia|\n",
      "|        14|        M14|         17|     GG1919|     Germany|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel|\n",
      "|        16|        M16|         23|     AC1000|      Turkey|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia|\n",
      "|        19|        M19|         14|     GG1919|      Canada|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM HVAC WHERE BuildingAge >= 10').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|HVACproduct|count(1)|\n",
      "+-----------+--------+\n",
      "|    ACMAX22|       3|\n",
      "|     AC1000|       3|\n",
      "|     JDNS77|       4|\n",
      "|     FN39TG|       4|\n",
      "|     GG1919|       4|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Can even mix DataFrame API with SQL:\n",
    "df.where('BuildingAge >= 10').createOrReplaceTempView('OldBuildings')\n",
    "spark.sql('SELECT HVACproduct, COUNT(*) FROM OldBuildings GROUP BY HVACproduct').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|HVACproduct|count|\n",
      "+-----------+-----+\n",
      "|    ACMAX22|    3|\n",
      "|     AC1000|    3|\n",
      "|     JDNS77|    4|\n",
      "|     FN39TG|    4|\n",
      "|     GG1919|    4|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1 = spark.sql('SELECT * FROM HVAC WHERE BuildingAge >= 10')\n",
    "d1.groupBy('HVACproduct').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|slen|\n",
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "|         1|         M1|         25|     AC1000|         USA|   5|\n",
      "|         2|         M2|         27|     FN39TG|      France|   8|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|   8|\n",
      "|         4|         M4|         17|     GG1919|     Finland|   9|\n",
      "|         5|         M5|          3|    ACMAX22|   Hong Kong|  11|\n",
      "|         6|         M6|          9|     AC1000|   Singapore|  11|\n",
      "|         7|         M7|         13|     FN39TG|South Africa|  14|\n",
      "|         8|         M8|         25|     JDNS77|   Australia|  11|\n",
      "|         9|         M9|         11|     GG1919|      Mexico|   8|\n",
      "|        10|        M10|         23|    ACMAX22|       China|   7|\n",
      "|        11|        M11|         14|     AC1000|     Belgium|   9|\n",
      "|        12|        M12|         26|     FN39TG|     Finland|   9|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia|  14|\n",
      "|        14|        M14|         17|     GG1919|     Germany|   9|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel|   8|\n",
      "|        16|        M16|         23|     AC1000|      Turkey|   8|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt|   7|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia|  11|\n",
      "|        19|        M19|         14|     GG1919|      Canada|   8|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina|  11|\n",
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UDF\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "slen = udf(lambda s: len(s)+2, IntegerType())\n",
    "df.select('*', slen(df['Country']).alias('slen')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|slen|\n",
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "|         1|         M1|         25|     AC1000|         USA|   3|\n",
      "|         2|         M2|         27|     FN39TG|      France|   6|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|   6|\n",
      "|         4|         M4|         17|     GG1919|     Finland|   7|\n",
      "|         5|         M5|          3|    ACMAX22|   Hong Kong|   9|\n",
      "|         6|         M6|          9|     AC1000|   Singapore|   9|\n",
      "|         7|         M7|         13|     FN39TG|South Africa|  12|\n",
      "|         8|         M8|         25|     JDNS77|   Australia|   9|\n",
      "|         9|         M9|         11|     GG1919|      Mexico|   6|\n",
      "|        10|        M10|         23|    ACMAX22|       China|   5|\n",
      "|        11|        M11|         14|     AC1000|     Belgium|   7|\n",
      "|        12|        M12|         26|     FN39TG|     Finland|   7|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia|  12|\n",
      "|        14|        M14|         17|     GG1919|     Germany|   7|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel|   6|\n",
      "|        16|        M16|         23|     AC1000|      Turkey|   6|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt|   5|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia|   9|\n",
      "|        19|        M19|         14|     GG1919|      Canada|   6|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina|   9|\n",
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register('slen', lambda s: len(s), IntegerType())\n",
    "spark.sql('SELECT *, slen(Country) AS slen FROM HVAC').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Flexible Data Model\n",
    "\n",
    "Sample data file at\n",
    "\n",
    "https://www.cse.ust.hk/msbd5003/data/products.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dimensions: struct (nullable = true)\n",
      " |    |-- height: double (nullable = true)\n",
      " |    |-- length: double (nullable = true)\n",
      " |    |-- width: double (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- warehouseLocation: struct (nullable = true)\n",
      " |    |-- latitude: double (nullable = true)\n",
      " |    |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json('../data/products.json')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+----------------+-----+-----------+-----------------+\n",
      "|    dimensions| id|            name|price|       tags|warehouseLocation|\n",
      "+--------------+---+----------------+-----+-----------+-----------------+\n",
      "|[9.5,7.0,12.0]|  2|An ice sculpture| 12.5|[cold, ice]|    [-78.75,20.4]|\n",
      "| [1.0,3.1,1.0]|  3|    A blue mouse| 25.5|       null|     [54.4,-32.7]|\n",
      "+--------------+---+----------------+-----+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   9.5|\n",
      "|   1.0|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accessing nested fields\n",
    "\n",
    "df.select(df['dimensions.height']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   9.5|\n",
      "|   1.0|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('dimensions.height').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   9.5|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('dimensions.height')\\\n",
    "  .filter(\"tags[0] = 'cold' AND warehouseLocation.latitude < 0\")\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dimensions=Row(height=9.5, length=7.0, width=12.0), id=2, name=u'An ice sculpture', price=12.5, tags=[u'cold', u'ice'], warehouseLocation=Row(latitude=-78.75, longitude=20.4)), Row(dimensions=Row(height=1.0, length=3.1, width=1.0), id=3, name=u'A blue mouse', price=25.5, tags=None, warehouseLocation=Row(latitude=54.4, longitude=-32.7))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Converting between RDD and DataFrame\n",
    "\n",
    "Sample data file at:\n",
    "\n",
    "https://www.cse.ust.hk/msbd5003/data/people.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Michael', 29), (u'Andy', 30), (u'Justin', 19)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"../data/people.txt\")\n",
    "\n",
    "def parse(l):\n",
    "    a = l.split(',')\n",
    "    return (a[0], int(a[1]))\n",
    "\n",
    "rdd = lines.map(parse)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|     _1| _2|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame from an RDD of tuples, schema is inferred\n",
    "df = spark.createDataFrame(rdd)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame from an RDD of tuples with column names, type is inferred\n",
    "df = spark.createDataFrame(rdd, ['name', 'age'])\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 29|Michael|\n",
      "| 30|   Andy|\n",
      "| 19| Justin|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame from an RDD of Rows, type is given in the Row objects\n",
    "from pyspark.sql import Row\n",
    "\n",
    "rdd_rows = rdd.map(lambda p: Row(name = p[0], age = p[1]))\n",
    "df = spark.createDataFrame(rdd_rows)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age| name|\n",
      "+----+-----+\n",
      "|  11|Alice|\n",
      "|null|  Bob|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Row fields with types incompatible with that of previous rows will be turned into nulls\n",
    "row1 = Row(name=\"Alice\", age=11)\n",
    "row2 = Row(name=\"Bob\", age='12')\n",
    "rdd_rows = sc.parallelize([row1, row2])\n",
    "df1 = spark.createDataFrame(rdd_rows)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Name: Justin']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd returns the content as an RDD of Rows\n",
    "teenagers = df.filter('age >= 13 and age <= 19')\n",
    "\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name)\n",
    "teenNames.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "DataFrames are stored using columnar storage with compression\n",
    "\n",
    "RDDs are stored using row storage without compression\n",
    "\n",
    "The RDD view of DataFrame just provides an interface, the Row objects are constructed on the fly and do not necessarily represent the internal storage format of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closure in DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  3|  3|\n",
      "|  4|  4|\n",
      "|  5|  5|\n",
      "|  6|  6|\n",
      "|  7|  7|\n",
      "|  8|  8|\n",
      "|  9|  9|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = range(10)\n",
    "df = spark.createDataFrame(zip(data, data))\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  3|  3|\n",
      "|  4|  4|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The 'closure' behaviour in RDD doesn't seem to exist for DataFrames\n",
    "\n",
    "x = 5\n",
    "df1 = df.filter(df._1 < x)\n",
    "df1.show()\n",
    "x = 3\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Filter (isnotnull(_1#773L) && (_1#773L < 5))\n",
      "+- Scan ExistingRDD[_1#773L,_2#774L]\n"
     ]
    }
   ],
   "source": [
    "# Because of the Catalyst optimizer !\n",
    "\n",
    "df1.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Project [((_1#773L * 2) + 4) AS ((((_1 * 2) + 2) + 1) + 1)#794L]\n",
      "+- Scan ExistingRDD[_1#773L,_2#774L]\n"
     ]
    }
   ],
   "source": [
    "def f():\n",
    "    return x/2\n",
    "x = 5\n",
    "df1 = df.select(df._1 * 2 + f() + 1 + 1)\n",
    "df1.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "x = 5\n",
    "a = rdd.filter(lambda z: z < x)\n",
    "print a.take(10)\n",
    "x = 3\n",
    "print a.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += 1\n",
    "\n",
    "df.foreach(increment_counter)\n",
    "\n",
    "print counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
